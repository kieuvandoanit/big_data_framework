version: '3.8'

networks:
  bigdata-net:
    driver: bridge

volumes:
  namenode:
  datanode1:
  datanode2:
  mysql_data:
  mysql_result_data:
  zookeeper_data:
  nifi_conf:
  nifi_content_repository:
  nifi_database_repository:
  nifi_flowfile_repository:
  nifi_provenance_repository:
  hive_metastore:

services:

  # MySQL cho Hive Metastore
  mysql:
    image: mysql:8.0
    container_name: mysql
    env_file: .env
    environment:
      MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD}
      MYSQL_DATABASE: ${MYSQL_DATABASE}
      MYSQL_USER: ${MYSQL_USER}
      MYSQL_PASSWORD: ${MYSQL_PASSWORD}
    ports:
      - "${MYSQL_PORT}:3306"
    volumes:
      - mysql_data:/var/lib/mysql
    networks:
      - bigdata-net
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost"]
      interval: 10s
      timeout: 5s
      retries: 5

  # MySQL Server cho lưu kết quả (Yêu cầu 7)
  mysql-result:
    image: mysql:8.0
    container_name: mysql-result
    env_file: .env
    environment:
      MYSQL_ROOT_PASSWORD: ${MYSQL_RESULT_ROOT_PASSWORD}
      MYSQL_DATABASE: ${MYSQL_RESULT_DATABASE}
      MYSQL_USER: ${MYSQL_RESULT_USER}
      MYSQL_PASSWORD: ${MYSQL_RESULT_PASSWORD}
    ports:
      - "${MYSQL_RESULT_PORT}:3306"
    volumes:
      - mysql_result_data:/var/lib/mysql
    networks:
      - bigdata-net
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Zookeeper 3.8.4
  zookeeper:
    image: zookeeper:3.8.4
    container_name: zookeeper
    environment:
      ZOO_MY_ID: 1
      ZOO_SERVERS: server.1=zookeeper:2888:3888;2181
      ZOO_TICK_TIME: 2000
      ZOO_INIT_LIMIT: 10
      ZOO_SYNC_LIMIT: 5
    ports:
      - "2181:2181"
    volumes:
      - zookeeper_data:/data
    networks:
      - bigdata-net
    healthcheck:
      test: ["CMD", "zkServer.sh", "status"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Hadoop Namenode - BDE2020
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    hostname: namenode
    # platform: linux/amd64
    env_file: .env
    environment:
      CLUSTER_NAME: ${CLUSTER_NAME}
      CORE_CONF_fs_defaultFS: hdfs://namenode:8020
      HDFS_CONF_dfs_namenode_http_address: 0.0.0.0:9870
      HDFS_CONF_dfs_namenode_rpc_address: 0.0.0.0:8020
      HDFS_CONF_dfs_http_policy: HTTP_ONLY
      HDFS_CONF_dfs_webhdfs_enabled: "true"
      HDFS_CONF_dfs_replication: ${HDFS_TOTAL_REPLICA}
    ports:
      - "50070:9870"
      - "8020:8020"
    volumes:
      - namenode:/hadoop/dfs/name
    networks:
      - bigdata-net
    # ADD healthcheck
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9870"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Hadoop Datanode 1 - BDE2020
  datanode1:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode1
    hostname: datanode1
    # platform: linux/amd64
    env_file: .env
    depends_on:
      namenode:
        condition: service_healthy # Wait for namenode to be healthy
    environment:
      CORE_CONF_fs_defaultFS: hdfs://namenode:8020
      HDFS_CONF_dfs_webhdfs_enabled: "true"
      HDFS_CONF_dfs_replication: ${HDFS_TOTAL_REPLICA}
      HDFS_CONF_dfs_datanode_hostname: datanode1
      HDFS_CONF_dfs_datanode_http_address: 0.0.0.0:9864
    ports:
      - "50071:9864"
    volumes:
      - datanode1:/hadoop/dfs/data
    networks:
      - bigdata-net

  # Hadoop Datanode 2 - BDE2020
  datanode2:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode2
    hostname: datanode2
    # platform: linux/amd64
    env_file: .env
    depends_on:
      namenode:
        condition: service_healthy # Wait for namenode to be healthy
    environment:
      CORE_CONF_fs_defaultFS: hdfs://namenode:8020
      HDFS_CONF_dfs_webhdfs_enabled: "true"
      HDFS_CONF_dfs_replication: ${HDFS_TOTAL_REPLICA}
      HDFS_CONF_dfs_datanode_hostname: datanode2
      HDFS_CONF_dfs_datanode_http_address: 0.0.0.0:9864
    ports:
      - "50072:9864"
    volumes:
      - datanode2:/hadoop/dfs/data
    networks:
      - bigdata-net

  # Service init HDFS directories
  hdfs-init:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: hdfs-init
    platform: linux/amd64
    env_file: .env
    depends_on:
      namenode:
        condition: service_healthy
      datanode1:
        condition: service_started
      datanode2:
        condition: service_started
    environment:
      CORE_CONF_fs_defaultFS: hdfs://namenode:8020
    networks:
      - bigdata-net
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        echo "Waiting for HDFS to be ready..."
        sleep 30
        
        echo "Testing HDFS connection..."
        for i in 1 2 3 4 5; do
          if hdfs dfs -ls / > /dev/null 2>&1; then
            echo "HDFS is ready!"
            break
          fi
          echo "Attempt $i: HDFS not ready yet, waiting..."
          sleep 10
        done
        
        echo "Creating HDFS directories..."
        hdfs dfs -mkdir -p /data 2>&1 | tee /tmp/mkdir.log
        hdfs dfs -mkdir -p /spark-logs 2>&1 | tee -a /tmp/mkdir.log
        hdfs dfs -mkdir -p /user/hive/warehouse 2>&1 | tee -a /tmp/mkdir.log
        
        echo "Setting permissions..."
        hdfs dfs -chmod 777 /data 2>&1 | tee /tmp/chmod.log
        hdfs dfs -chmod 777 /spark-logs 2>&1 | tee -a /tmp/chmod.log
        hdfs dfs -chmod 777 /user/hive/warehouse 2>&1 | tee -a /tmp/chmod.log
        
        echo "Verifying directories..."
        hdfs dfs -ls / 2>&1 | tee /tmp/verify.log
        
        echo "HDFS initialization completed!"
        cat /tmp/mkdir.log /tmp/chmod.log /tmp/verify.log
    restart: "no"

  # YARN ResourceManager - BDE2020
  resourcemanager:
    image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8
    container_name: resourcemanager
    hostname: resourcemanager
    platform: linux/amd64
    env_file: .env
    depends_on:
      - namenode
    environment:
      CORE_CONF_fs_defaultFS: hdfs://namenode:8020
      YARN_CONF_yarn_resourcemanager_hostname: resourcemanager
    ports:
      - "8088:8088"
      - "8032:8032"
    networks:
      - bigdata-net

  # YARN NodeManager 1 - BDE2020
  nodemanager1:
    image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8
    container_name: nodemanager1
    hostname: nodemanager1
    platform: linux/amd64
    env_file: .env
    depends_on:
      - resourcemanager
    environment:
      CORE_CONF_fs_defaultFS: hdfs://namenode:8020
      YARN_CONF_yarn_resourcemanager_hostname: resourcemanager
    ports:
      - "8042:8042"
    networks:
      - bigdata-net

  # YARN NodeManager 2 - BDE2020
  nodemanager2:
    image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8
    container_name: nodemanager2
    hostname: nodemanager2
    platform: linux/amd64
    env_file: .env
    depends_on:
      - resourcemanager
    environment:
      CORE_CONF_fs_defaultFS: hdfs://namenode:8020
      YARN_CONF_yarn_resourcemanager_hostname: resourcemanager
    ports:
      - "8043:8042"
    networks:
      - bigdata-net

  # Hive Server - BDE2020
  hive-server:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-server
    hostname: hive-server
    platform: linux/amd64
    env_file: .env
    depends_on:
      mysql:
        condition: service_healthy
      namenode:
        condition: service_started
      hdfs-init:
        condition: service_completed_successfully
    environment:
      HIVE_CORE_CONF_javax_jdo_option_ConnectionURL: jdbc:mysql://mysql:3306/${MYSQL_DATABASE}?createDatabaseIfNotExist=true&useSSL=false
      HIVE_CORE_CONF_javax_jdo_option_ConnectionDriverName: com.mysql.cj.jdbc.Driver
      HIVE_CORE_CONF_javax_jdo_option_ConnectionUserName: ${MYSQL_USER}
      HIVE_CORE_CONF_javax_jdo_option_ConnectionPassword: ${MYSQL_PASSWORD}
      CORE_CONF_fs_defaultFS: hdfs://namenode:8020
      HIVE_SITE_CONF_hive_metastore_uris: thrift://hive-server:9083
    ports:
      - "10000:10000"
      - "10002:10002"
      - "9083:9083"
    networks:
      - bigdata-net

  # Apache NiFi
  nifi:
    image: apache/nifi:latest
    container_name: nifi
    hostname: nifi
    depends_on:
      namenode:
        condition: service_healthy
    environment:
      # CHỈ DÙNG HTTPS
      NIFI_WEB_HTTP_PORT: ""
      NIFI_WEB_HTTPS_HOST: 0.0.0.0
      NIFI_WEB_HTTPS_PORT: 8443

      NIFI_WEB_PROXY_HOST: "113.161.88.63:9000,localhost:8443"
      NIFI_WEB_HTTPS_SNI_HOST_CHECK: 'false'

      # Single User Auth
      SINGLE_USER_CREDENTIALS_USERNAME: admin
      SINGLE_USER_CREDENTIALS_PASSWORD: adminadmin123

      # JVM Settings
      NIFI_JVM_HEAP_INIT: 512m
      NIFI_JVM_HEAP_MAX: 1g

      # Disable clustering
      NIFI_CLUSTER_IS_NODE: 'false'
    ports:
      - "9000:8443"    # map 9000 ngoài host vào 8443 trong container
    volumes:
      - nifi_conf:/opt/nifi/nifi-current/conf
      - nifi_content_repository:/opt/nifi/nifi-current/content_repository
      - nifi_database_repository:/opt/nifi/nifi-current/database_repository
      - nifi_flowfile_repository:/opt/nifi/nifi-current/flowfile_repository
      - nifi_provenance_repository:/opt/nifi/nifi-current/provenance_repository

      - ./nifi-ext:/opt/nifi/nifi-current/nar_extensions

      - ./hadoop-conf:/opt/nifi/hadoop-conf

      - ./realtime-data:/home/hduser/realtime-data
    networks:
      - bigdata-net
    healthcheck:
      test: ["CMD-SHELL", "curl -k -f https://localhost:8443/nifi || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 180s

  # Spark Master - BDE2020
  spark-master:
    image: bde2020/spark-master:3.3.0-hadoop3.3
    container_name: spark-master
    hostname: spark-master
    platform: linux/amd64
    env_file: .env
    environment:
      SPARK_MASTER_HOST: spark-master
      SPARK_NO_DAEMONIZE: "true"
      SPARK_PUBLIC_DNS: ${PUBLIC_DNS}
    ports:
      - "${SPARK_MASTER_HTTP_PORT}:8080"
      - "7077:7077"
    networks:
      - bigdata-net

  # Spark Worker 1 - BDE2020
  spark-worker1:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    container_name: spark-worker1
    hostname: spark-worker1
    platform: linux/amd64
    env_file: .env
    depends_on:
      - spark-master
    environment:
      SPARK_MASTER: spark://spark-master:7077
      SPARK_WORKER_WEBUI_PORT: 8081
      SPARK_WORKER_CORES: 4
      SPARK_WORKER_MEMORY: 4g
      SPARK_PUBLIC_DNS: ${PUBLIC_DNS}
    ports:
      - "${SPARK_WORKER_1_HTTP_PORT}:8081"
    networks:
      - bigdata-net

  # Spark Worker 2 - BDE2020
  spark-worker2:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    container_name: spark-worker2
    hostname: spark-worker2
    platform: linux/amd64
    env_file: .env
    depends_on:
      - spark-master
    environment:
      SPARK_MASTER: spark://spark-master:7077
      SPARK_WORKER_WEBUI_PORT: 8081
      SPARK_WORKER_CORES: 4
      SPARK_WORKER_MEMORY: 4g
      SPARK_PUBLIC_DNS: ${PUBLIC_DNS}
    ports:
      - "${SPARK_WORKER_2_HTTP_PORT}:8081"
    networks:
      - bigdata-net

  # Spark History Server - BDE2020
  spark-history-server:
    image: bde2020/spark-history-server:3.3.0-hadoop3.3
    container_name: spark-history-server
    hostname: spark-history-server
    platform: linux/amd64
    env_file: .env
    depends_on:
      - namenode
    environment:
      SPARK_HISTORY_OPTS: "-Dspark.history.fs.logDirectory=hdfs://namenode:8020/spark-logs"
    ports:
      - "18080:18080"
    networks:
      - bigdata-net
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        echo "Waiting for HDFS..."
        sleep 20
        echo "Verifying /spark-logs directory..."
        hdfs dfs -test -d /spark-logs || hdfs dfs -mkdir -p /spark-logs
        echo "Starting Spark History Server..."
        /spark/bin/spark-class org.apache.spark.deploy.history.HistoryServer

  data-simulator:
      image: python:3.9-slim
      container_name: data-simulator
      working_dir: /app
      volumes:
        # Mount Python scripts
        - ./data_simulator.py:/app/data_simulator.py
        - ./generate_sample_data.py:/app/generate_sample_data.py
        
        # Mount data directories (read-write)
        - ./data:/home/hduser/data
        - ./realtime-data:/home/hduser/realtime-data
      networks:
        - bigdata-net
      # Keep container running để có thể exec vào chạy scripts
      command: tail -f /dev/null
      restart: unless-stopped
